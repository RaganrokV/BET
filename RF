# -*- coding: utf-8 -*-
import pickle

import pandas as pd

import random
from matplotlib import pyplot as plt
from scipy.interpolate import lagrange
import numpy as np
import torch

import joblib
from My_utils.evaluation_scheme import evaluation
from My_utils.preprocess_data import  divide_data

from sklearn.ensemble import RandomForestRegressor
import warnings
warnings.filterwarnings("ignore")
import time
#%%
def process_data(car_data,soc_min,soc_max,ava_len):
    """abnormal values"""
    Veh_state = car_data[car_data['车辆状态'].isin([1])]  # 1 indicate run
    Run_state = Veh_state[Veh_state['运行模式'].isin([1])]  # 1 indicate pure electric
    Nocharge = Run_state[Run_state['充电状态'].isin([3])]  #
    NO_min = Nocharge[Nocharge['SOC'] >= soc_min]
    NO_max = NO_min[NO_min['SOC'] <= soc_max]
    '''search'''
    data = NO_max['SOC'].values
    dif_data = np.diff(data)
    index = np.where(dif_data > 5)  # recharge exist so at least 20%
    index_2 = index[0].tolist()
    index_2.insert(0, 0)
    index_2.insert(-1, -1)

    my_data = np.nan_to_num(NO_max.values[:, :-9])

    # %
    snapshot = []
    for i in range(len(index_2) - 2):
        snapshot.append(my_data[index_2[i]:index_2[i + 1], :])

    # %
    del_index = []
    for i in range(len(snapshot)):
        length = len(snapshot[i])
        if length <= ava_len :
            del_index.append(i)
        else:
            pass

    for i in range(len(del_index)):
        del snapshot[del_index[i] - i]  # del snapshot that less than 100

    return snapshot



#%%
# data=np.load('4-SOC_prediction/car_data.npy',allow_pickle=True)
data=np.load('4-SOC_prediction/car_data_B.npy',allow_pickle=True)

car_data1=data[0]
car_data2=data[1]
car_data3=data[2]
car_data4=data[3]
car_data5=data[4]


snapshot1=process_data(car_data1,10,90,180)
snapshot2=process_data(car_data2,10,90,180)
snapshot3=process_data(car_data3,10,90,180)
snapshot4=process_data(car_data4,10,90,180)
snapshot5=process_data(car_data5,10,90,180)
#%%
"""MCS"""
np.random.seed(42)
random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)
def monte_carlo_sampling(lst, num_samples):
    rand_idx = np.random.choice(len(lst), num_samples, replace=False)
    samples = [lst[i] for i in rand_idx]
    # 在原始list中删除采样样本
    lst = [item for i, item in enumerate(lst) if i not in rand_idx]

    return samples,lst

num_samples=20

samples1, lst1 = monte_carlo_sampling(snapshot1, num_samples)
samples2, lst2 = monte_carlo_sampling(snapshot2, num_samples)
samples3, lst3 = monte_carlo_sampling(snapshot3, num_samples)
samples4, lst4 = monte_carlo_sampling(snapshot4, num_samples)
samples5, lst5 = monte_carlo_sampling(snapshot5, num_samples)

#%%
# snapshot=snapshot1+snapshot2+snapshot3+snapshot4+snapshot5
snapshot=lst1+lst2+lst3+lst4+lst5
"""打乱5辆车"""
np.random.seed(42)
random.seed(42)
random.shuffle(snapshot)
data=np.vstack(snapshot)
data_no_time=data[:,1:].astype(float)


#%%

with open('4-SOC_prediction/models/coFUN_A.pkl', 'rb') as file:
    efficiency_matrix_A = pickle.load(file)

with open('4-SOC_prediction/models/coFUN_B.pkl', 'rb') as file:
    efficiency_matrix_B = pickle.load(file)


def ECR_A(speed, soc):
    if speed <= 110 and soc >= 10 and soc <= 90:
        return efficiency_matrix_A[speed, soc - 10]  # Return the average efficiency if outside the range
    else:
        return np.mean(efficiency_matrix_A)


def ECR_B(speed, soc):
    if speed <= 122 and soc >= 10 and soc <= 90:
        return efficiency_matrix_B[speed, soc - 10]  # Return the average efficiency if outside the range
    else:
        return np.mean(efficiency_matrix_B)
# %%
Capacity_A = 60
Capacity_B = 87
ave_ECR_A=0.21
ave_ECR_B=0.22
Range_limt_A=270
Range_limt_B=360
#%%
def feature_formation(snapshot, ave_EC, Capacity, limt, coFUN):
    """feature formation"""

    """(0)RDR"""
    R = []
    for i in range(len(snapshot)):
        temp = snapshot[i][1:-1, 1:].astype(float)  # exclude timestamp and first/last point
        r = (temp[-1, 5] - temp[:, 5] + temp[-1, 8] * Capacity / ave_EC / 100)

        R.append(r)  # no estimated res

    RDR = np.hstack(R)
    indices = np.where(RDR > limt)

    RDR = np.delete(RDR, indices[0], axis=0)
    RDR_mean = np.mean(RDR)
    RDR_std = np.std(RDR)
    RDR_Z = (RDR - RDR_mean) / RDR_std
    #
    """(1)speed"""
    SP = []
    for i in range(len(snapshot)):
        temp = snapshot[i][1:-1, 1:]  # exclude timestamp and first/last point
        speed = temp[:, 1].astype(float)
        zeros_idx = np.where(speed == 0)[0]
        for idx in zeros_idx:
            # 找到前后不为0的值所在位置
            left_idx = idx - 1
            while left_idx >= 0 and speed[left_idx] == 0:
                left_idx -= 1
            right_idx = idx + 1
            while right_idx < len(speed) and speed[right_idx] == 0:
                right_idx += 1
            # 如果前后都有非0值，则进行插值替换
            if left_idx >= 0 and right_idx < len(speed):
                x = np.array([left_idx, right_idx])
                y = speed[x]
                speed[idx] = lagrange(x, y)(idx)

        SP.append(speed)  #

    SP = np.hstack(SP)
    SP = np.delete(SP, indices[0], axis=0)
    SP_mean = np.mean(SP)
    SP_std = np.std(SP)
    SP_Z = (SP - SP_mean) / SP_std
    #

    """(2)SOC  ni linspace"""
    S = []
    for i in range(len(snapshot)):
        temp = snapshot[i][1:-1, 1:]  # exclude timestamp and first/last point
        s = temp[:, 8].astype(float)

        S.append(s)  #

    SOC = np.hstack(S)
    SOC = np.delete(SOC, indices[0], axis=0)

    SOC_Z = SOC/100

    #
    """(3)SOH"""
    M = []
    for i in range(len(snapshot)):
        temp = snapshot[i][1:-1, 1:]  # exclude timestamp and first/last point
        m = 0.8 + 0.2 * (1 - temp[:, 5] / 1500000)
        M.append(m)  #
    SOH = np.hstack(M)
    SOH = np.delete(SOH, indices[0], axis=0)

    SOH_Z = SOH

    """(4)SOE"""
    SOE_Z = SOC/100 * SOH

    #
    """(5)ECR"""
    if coFUN == 1:
        func = ECR_A
    else:
        func = ECR_B
    ecr = []
    for i in range(len(SP)):
        ecr.append(func(np.ceil(SP.reshape(-1, 1)[i]).astype(int), SOC.reshape(-1, 1)[i].astype(int)))

    ECR = np.hstack(ecr)

    ECR_mean = np.mean(ECR, axis=0)
    ECR_std = np.std(ECR, axis=0)
    ECR_Z = (ECR - ECR_mean) / ECR_std
    #
    """(6)auxiliary info"""
    A = []  # 电压电流 单体最大最小电压 最大最小温度
    for i in range(len(snapshot)):
        temp = snapshot[i][1:-1, 1:]  # exclude timestamp and first/last point
        a = temp[:, [6, 7, 16, 19, 22, 25]].astype(float)
        A.append(a)  # no estimated res

    AU = np.vstack(A)
    AU = np.delete(AU, indices[0], axis=0)
    AU_mean = np.mean(AU, axis=0)
    AU_std = np.std(AU, axis=0)
    AU_Z = (AU - AU_mean) / AU_std

    # Construct the feature matrix
    Data_set = np.concatenate((RDR_Z.reshape(-1, 1),
                               ECR_Z.reshape(-1, 1),
                               SOE_Z.reshape(-1, 1),
                               SOH_Z.reshape(-1, 1),
                               SOC_Z.reshape(-1, 1),
                               SP_Z.reshape(-1, 1),
                               AU_Z), axis=1).astype(float)

    return Data_set,RDR_std,RDR_mean

#%%
Data_set,_,_ = feature_formation(snapshot, ave_ECR_A, Capacity_A, Range_limt_A, 1)
# Data_set,_,_ = feature_formation(snapshot, ave_ECR_B, Capacity_B, Range_limt_B, 2)

# %%
# set seed for reproductive 42 is the answer to the universe
"""hyper paras:"""
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)

train_size=int(0.9*len(Data_set))
seq_len=1 #14+1
pre_len=1


#%%
#NOTE testset indicate the validation
trainX=torch.tensor(Data_set[:train_size,1:]).unsqueeze(2).float()
trainY=torch.tensor(Data_set[:train_size,0]).unsqueeze(1).unsqueeze(2).float()
testX=torch.tensor(Data_set[train_size+1:,1:]).unsqueeze(2).float()
testY=torch.tensor(Data_set[train_size+1:,0]).unsqueeze(1).unsqueeze(2).float()

trainX, trainY = np.array(trainX).squeeze(), np.array(trainY).squeeze()

testX, testY = np.array(testX).squeeze(), np.array(testY).squeeze()
#%%
start = time.time()

RF = RandomForestRegressor(n_estimators=50, max_depth=5,
                           random_state=42, criterion='squared_error')
Trained_RF = RF.fit(trainX, trainY.reshape(-1, 1))


end = time.time()

print (str(end-start))
#%%
start = time.time()

sample_snapshot=samples1+samples2+samples3+samples4+samples5

car,RDR_std,RDR_mean = feature_formation(sample_snapshot, ave_ECR_A, Capacity_A, Range_limt_A, 1)
# car,RDR_std,RDR_mean = feature_formation(sample_snapshot, ave_ECR_B, Capacity_B, Range_limt_B, 2)


trainX1 = torch.tensor(car[:, 1:]).unsqueeze(2).float()
trainY1 = torch.tensor(car[:, 0]).unsqueeze(1).unsqueeze(2).float()

X, Y = np.array(trainX1).squeeze(), np.array(trainY1).squeeze()
Norm_pred = Trained_RF.predict(X)

Preds = Norm_pred.reshape(-1, 1) * RDR_std + RDR_mean

GT = Y.reshape(-1, 1) * RDR_std + RDR_mean


evaluation(GT, Preds)
end = time.time()

# print (str(end-start))
print ((end-start)/len(GT)*100000)
evaluation(GT, Preds)

#%% train
sample_snapshot=samples1+samples2+samples3+samples4+samples5

car,RDR_std,RDR_mean = feature_formation(sample_snapshot, ave_ECR_A, Capacity_A, Range_limt_A, 1)
# car,RDR_std,RDR_mean = feature_formation(sample_snapshot, ave_ECR_B, Capacity_B, Range_limt_B, 2)


trainX1 = torch.tensor(car[:, 1:]).unsqueeze(2).float()
trainY1 = torch.tensor(car[:, 0]).unsqueeze(1).unsqueeze(2).float()

X, Y = np.array(trainX1).squeeze(), np.array(trainY1).squeeze()

"""prediction"""
R=[]
for N in [20,30,40,50,60]:
    RF = RandomForestRegressor(n_estimators=N, max_depth=5,
                               random_state=42, criterion='squared_error')
    Trained_RF = RF.fit(trainX, trainY.reshape(-1, 1))

    Norm_pred = Trained_RF.predict(X)

    Preds = Norm_pred.reshape(-1, 1) * RDR_std + RDR_mean

    GT = Y.reshape(-1, 1) * RDR_std + RDR_mean

    R.append(evaluation(GT, Preds)[0])


print('finish')
#%%
AResult_array = np.vstack(R)
#%%
joblib.dump(Trained_RF, '4-SOC_prediction/models/Trained_RF.pkl')
# joblib.dump(Trained_RF, '4-SOC_prediction/models/Trained_RF_B.pkl')
#load model
# Trained_RF = joblib.load('4-SOC_prediction/models/Trained_RF.pkl')


#%%
"""RF"""
# Trained_RF = joblib.load('4-SOC_prediction/models/Trained_RF.pkl')
Trained_RF_B = joblib.load('4-SOC_prediction/models/Trained_RF_B.pkl')


sample_snapshot=samples1+samples2+samples3+samples4+samples5

# car,RDR_std,RDR_mean = feature_formation(sample_snapshot, ave_ECR_A, Capacity_A, Range_limt_A, 1)
car,RDR_std,RDR_mean = feature_formation(sample_snapshot, ave_ECR_B, Capacity_B, Range_limt_B, 2)


trainX1 = torch.tensor(car[:, 1:]).unsqueeze(2).float()
trainY1 = torch.tensor(car[:, 0]).unsqueeze(1).unsqueeze(2).float()

X, Y = np.array(trainX1).squeeze(), np.array(trainY1).squeeze()

# Norm_pred = Trained_RF.predict(X)
Norm_pred = Trained_RF_B.predict(X)

Preds = Norm_pred.reshape(-1, 1) * RDR_std + RDR_mean

GT = Y.reshape(-1, 1) * RDR_std + RDR_mean

evaluation(GT, Preds)
