# -*- coding: utf-8 -*-
import math
import time
import random
from matplotlib import pyplot as plt
from scipy.interpolate import lagrange
import numpy as np
import torch
import torch.nn as nn
import pickle
import seaborn as sns
from My_utils.evaluation_scheme import evaluation
import torch.utils.data as Data
from torch.utils.data import RandomSampler, DataLoader
import pytorch_warmup as warmup

#%%
def process_data(car_data,soc_min,soc_max,ava_len):
    """abnormal values"""
    Veh_state = car_data[car_data['车辆状态'].isin([1])]  # 1 indicate run
    Run_state = Veh_state[Veh_state['运行模式'].isin([1])]  # 1 indicate pure electric
    Nocharge = Run_state[Run_state['充电状态'].isin([3])]  #
    NO_min = Nocharge[Nocharge['SOC'] >= soc_min]
    NO_max = NO_min[NO_min['SOC'] <= soc_max]
    '''search'''
    data = NO_max['SOC'].values
    dif_data = np.diff(data)
    index = np.where(dif_data > 5)  # recharge exist so at least 20%
    index_2 = index[0].tolist()
    index_2.insert(0, 0)
    index_2.insert(-1, -1)

    my_data = np.nan_to_num(NO_max.values[:, :-9])

    # %
    snapshot = []
    for i in range(len(index_2) - 2):
        snapshot.append(my_data[index_2[i]:index_2[i + 1], :])

    # %
    del_index = []
    for i in range(len(snapshot)):
        length = len(snapshot[i])
        if length <= ava_len :
            del_index.append(i)
        else:
            pass

    for i in range(len(del_index)):
        del snapshot[del_index[i] - i]  # del snapshot that less than 100

    return snapshot
#%%
# data=np.load('4-SOC_prediction/car_data.npy',allow_pickle=True)
data=np.load('4-SOC_prediction/car_data_B.npy',allow_pickle=True)

car_data1=data[0]
car_data2=data[1]
car_data3=data[2]
car_data4=data[3]
car_data5=data[4]


snapshot1=process_data(car_data1,10,90,180)
snapshot2=process_data(car_data2,10,90,180)
snapshot3=process_data(car_data3,10,90,180)
snapshot4=process_data(car_data4,10,90,180)
snapshot5=process_data(car_data5,10,90,180)

#%%
"""MCS"""
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)
def monte_carlo_sampling(lst, num_samples):
    rand_idx = np.random.choice(len(lst), num_samples, replace=False)
    samples = [lst[i] for i in rand_idx]
    # 在原始list中删除采样样本
    lst = [item for i, item in enumerate(lst) if i not in rand_idx]

    return samples,lst

num_samples=20

samples1, lst1 = monte_carlo_sampling(snapshot1, num_samples)
samples2, lst2 = monte_carlo_sampling(snapshot2, num_samples)
samples3, lst3 = monte_carlo_sampling(snapshot3, num_samples)
samples4, lst4 = monte_carlo_sampling(snapshot4, num_samples)
samples5, lst5 = monte_carlo_sampling(snapshot5, num_samples)

#%%
snapshot=lst1+lst2+lst3+lst4+lst5
"""打乱5辆车"""
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)
random.shuffle(snapshot)
data=np.vstack(snapshot)
data_no_time=data[:,1:].astype(float)
#%%
with open('4-SOC_prediction/models/coFUN_A.pkl', 'rb') as file:
    efficiency_matrix_A = pickle.load(file)

with open('4-SOC_prediction/models/coFUN_B.pkl', 'rb') as file:
    efficiency_matrix_B = pickle.load(file)


def ECR_A(speed, soc):
    if speed <= 110 and soc >= 10 and soc <= 90:
        return efficiency_matrix_A[speed, soc - 10]  # Return the average efficiency if outside the range
    else:
        return np.mean(efficiency_matrix_A)


def ECR_B(speed, soc):
    if speed <= 122 and soc >= 10 and soc <= 90:
        return efficiency_matrix_B[speed, soc - 10]  # Return the average efficiency if outside the range
    else:
        return np.mean(efficiency_matrix_B)
# %%
Capacity_A = 60
Capacity_B = 87
ave_ECR_A=0.21
ave_ECR_B=0.22
Range_limt_A=270
Range_limt_B=360
#%%
def feature_formation(snapshot, ave_EC, Capacity, limt, coFUN):
    """feature formation"""

    """(0)RDR"""
    R = []
    for i in range(len(snapshot)):
        temp = snapshot[i][1:-1, 1:].astype(float)  # exclude timestamp and first/last point
        r = (temp[-1, 5] - temp[:, 5] + temp[-1, 8] * Capacity / ave_EC / 100)

        R.append(r)  # no estimated res

    RDR = np.hstack(R)
    indices = np.where(RDR > limt)

    RDR = np.delete(RDR, indices[0], axis=0)
    RDR_mean = np.mean(RDR)
    RDR_std = np.std(RDR)
    RDR_Z = (RDR - RDR_mean) / RDR_std
    #
    """(1)speed"""
    SP = []
    for i in range(len(snapshot)):
        temp = snapshot[i][1:-1, 1:]  # exclude timestamp and first/last point
        speed = temp[:, 1].astype(float)
        zeros_idx = np.where(speed == 0)[0]
        for idx in zeros_idx:
            # 找到前后不为0的值所在位置
            left_idx = idx - 1
            while left_idx >= 0 and speed[left_idx] == 0:
                left_idx -= 1
            right_idx = idx + 1
            while right_idx < len(speed) and speed[right_idx] == 0:
                right_idx += 1
            # 如果前后都有非0值，则进行插值替换
            if left_idx >= 0 and right_idx < len(speed):
                x = np.array([left_idx, right_idx])
                y = speed[x]
                speed[idx] = lagrange(x, y)(idx)

        SP.append(speed)  #

    SP = np.hstack(SP)
    SP = np.delete(SP, indices[0], axis=0)
    SP_mean = np.mean(SP)
    SP_std = np.std(SP)
    SP_Z = (SP - SP_mean) / SP_std
    #

    """(2)SOC  ni linspace"""
    S = []
    for i in range(len(snapshot)):
        temp = snapshot[i][1:-1, 1:]  # exclude timestamp and first/last point
        s = temp[:, 8].astype(float)

        S.append(s)  #

    SOC = np.hstack(S)
    SOC = np.delete(SOC, indices[0], axis=0)

    SOC_Z = SOC/100

    #
    """(3)SOH"""
    M = []
    for i in range(len(snapshot)):
        temp = snapshot[i][1:-1, 1:]  # exclude timestamp and first/last point
        m = 0.8 + 0.2 * (1 - temp[:, 5] / 1500000)
        M.append(m)  #
    SOH = np.hstack(M)
    SOH = np.delete(SOH, indices[0], axis=0)

    SOH_Z = SOH

    """(4)SOE"""
    SOE_Z = SOC/100 * SOH

    #
    """(5)ECR"""
    if coFUN == 1:
        func = ECR_A
    else:
        func = ECR_B
    ecr = []
    for i in range(len(SP)):
        ecr.append(func(np.ceil(SP.reshape(-1, 1)[i]).astype(int), SOC.reshape(-1, 1)[i].astype(int)))

    ECR = np.hstack(ecr)

    ECR_mean = np.mean(ECR, axis=0)
    ECR_std = np.std(ECR, axis=0)
    ECR_Z = (ECR - ECR_mean) / ECR_std
    #
    """(6)auxiliary info"""
    A = []  # 电压电流 单体最大最小电压 最大最小温度
    for i in range(len(snapshot)):
        temp = snapshot[i][1:-1, 1:]  # exclude timestamp and first/last point
        a = temp[:, [6, 7, 16, 19, 22, 25]].astype(float)
        A.append(a)  # no estimated res

    AU = np.vstack(A)
    AU = np.delete(AU, indices[0], axis=0)
    AU_mean = np.mean(AU, axis=0)
    AU_std = np.std(AU, axis=0)
    AU_Z = (AU - AU_mean) / AU_std

    # Construct the feature matrix
    Data_set = np.concatenate((RDR_Z.reshape(-1, 1),
                               ECR_Z.reshape(-1, 1),
                               SOE_Z.reshape(-1, 1),
                               SOH_Z.reshape(-1, 1),
                               SOC_Z.reshape(-1, 1),
                               SP_Z.reshape(-1, 1),
                               AU_Z), axis=1).astype(float)

    return Data_set,RDR_std,RDR_mean

#%%
# Data_set,_,_ = feature_formation(snapshot, ave_ECR_A, Capacity_A, Range_limt_A, 1)
Data_set,_,_ = feature_formation(snapshot, ave_ECR_B, Capacity_B, Range_limt_B, 2)

#%%
"""hyper paras:"""
# batch_size=128
batch_size=128

train_size=int(0.9*len(Data_set))

trainX=torch.tensor(Data_set[:train_size,1:]).unsqueeze(2).float()
trainY=torch.tensor(Data_set[:train_size,0]).unsqueeze(1).unsqueeze(2).float()

testX=torch.tensor(Data_set[train_size+1:,1:]).unsqueeze(2).float()
testY=torch.tensor(Data_set[train_size+1:,0]).unsqueeze(1).unsqueeze(2).float()


test_dataset = Data.TensorDataset(testX, testY)
dataloaders_valid= Data.DataLoader(dataset=test_dataset,
                                    batch_size=batch_size,
                                    shuffle=False)


#%%
trainXY=torch.cat((trainY,trainX),dim=1).squeeze()
sorted_trainXY = trainXY[trainXY[:, 0].argsort()]
mileage = trainXY[:, 0]
min_mileage = torch.min(mileage)
max_mileage = torch.max(mileage)

sorted_mileage = mileage[mileage.argsort()]
num_subsets = 20
step = (max_mileage - min_mileage) / num_subsets
# 计算边界值对应的索引
boundaries = [min_mileage + i * step for i in range(num_subsets + 1)]
boundaries_indices = [torch.where(sorted_mileage <= boundary)[0][-1] for boundary in boundaries]
boundaries_indices[0]=0
boundaries_indices[-1]=len(trainXY)
# 使用索引操作进行划分
subsets = []
for i in range(num_subsets):
    start_idx = boundaries_indices[i] + 1 if i > 0 else 0
    end_idx = boundaries_indices[i + 1] + 1
    subset = sorted_trainXY[start_idx:end_idx, :]
    sorted_indices = np.argsort(subset[:, 0])  # Get the indices that sort the first column
    sorted_subset = subset[sorted_indices, :]  # Use fancy indexing to sort the subset
    subsets.append(sorted_subset.squeeze(1))

#%%
"""Hierachical sample"""
# 定义采样函数--对少的进行重复采样，直达制定长度
def random_subset_sampler(subset, num_samples):
    subset_size = len(subset)
    if subset_size >= num_samples:
        indices = torch.randperm(subset_size)[:num_samples]
    else:
        # 重复采样直到达到指定的样本数
        repeat_times = num_samples // subset_size + 1
        indices = torch.cat([torch.randperm(subset_size) for _ in range(repeat_times)])
        indices = indices[:num_samples]
    return subset[indices]

# 设置采样参数
rate=1 #indicate the proportipn in total sample
num_subsets = 20
num_samples_per_subset = int(len(trainXY)/num_subsets*rate)
num_models = 5


"""new method"""

sampled_indices = []
    # 遍历每个子集
for subset in subsets:
        # 随机采样n个样本
    new_indices = random_subset_sampler(subset, num_samples_per_subset)
    sampled_indices.append(new_indices)
    # 将采样的样本拼接成一个新的张量

Balacned_sample = [torch.cat(sampled_indices[i:i+4]) for i in range(0, len(sampled_indices), 4)]


#%%
batch_size=128

dataloaders_train = []
for i in range(num_models):
    trainX=Balacned_sample[i][:,1:].unsqueeze(2)
    trainY = Balacned_sample[i][:, 0].unsqueeze(1).unsqueeze(2)

    train_dataset = Data.TensorDataset(trainX, trainY)
    train_loader = Data.DataLoader(dataset=train_dataset,
                                   batch_size=batch_size, shuffle=True,
                                   generator=torch.Generator().manual_seed(1))
    dataloaders_train.append(train_loader)


#%%
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() *
            (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0).transpose(0, 1)

        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(1), :].squeeze(1)
        return x

class TransformerTS(nn.Module):
    def __init__(self,
                 input_dim,
                 dec_seq_len,
                 out_seq_len,
                 d_model,
                 nhead,
                 num_encoder_layers,
                 num_decoder_layers,
                 dim_feedforward,
                 dropout,
                 activation,
                 custom_encoder=None,
                 custom_decoder=None):
        r"""
        Args:
            input_dim: dimision of imput series
            d_model: the number of expected features in the encoder/decoder inputs (default=512).
            nhead: the number of heads in the multiheadattention models (default=8).
            num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6).
            num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6).
            dim_feedforward: the dimension of the feedforward network model (default=2048).
            dropout: the dropout value (default=0.1).
            activation: the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu).
            custom_encoder: custom encoder (default=None).
            custom_decoder: custom decoder (default=None).


        """
        super(TransformerTS, self).__init__()
        self.transform = nn.Transformer(
            d_model=d_model,
            nhead=nhead,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation=activation,
            custom_encoder=custom_encoder,
            custom_decoder=custom_decoder,
        )
        self.pos = PositionalEncoding(d_model)
        self.enc_input_fc = nn.Linear(input_dim, d_model)
        self.dec_input_fc = nn.Linear(input_dim, d_model)
        # self.out_fc = nn.Linear(dec_seq_len * d_model, out_seq_len)
        self.out_fc = nn.Linear(d_model, out_seq_len)
        self.dec_seq_len = dec_seq_len

    def forward(self, x):
        x = x.transpose(2 ,1)
        # embedding
        embed_encoder_input = self.pos(self.enc_input_fc(x))
        embed_decoder_input = self.dec_input_fc(x[-self.dec_seq_len:, :])
        # transform
        x = self.transform(embed_encoder_input, embed_decoder_input)

        # output
        # x = x.transpose(1, 2)
        x = self.out_fc(x)
        return x[:,:,-1]
#%%
def print_grad_norms(model):
    total_norm = 0.0
    for name, param in model.named_parameters():
        if param.requires_grad and param.grad is not None:
            param_norm = param.grad.norm(2)
            total_norm += param_norm.item() ** 2
            print(f"{name}: {param_norm:.3f}")
    total_norm = total_norm ** 0.5
    print(f"Total norm: {total_norm:.3f}")

def count_parameters(model):
    """
    计算PyTorch模型的参数量。
    """
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

#%%
def train_model(T_model,dataloaders_train,dataloaders_valid,epochs,optimizer,criterion):
    best_val_loss = float("inf")
    best_model = None
    #           train
    train_loss_all = []
    T_model.train()  # Turn on the train mode
    total_loss = 0.

    for epoch in range(epochs):
        train_loss = 0
        train_num = 0
        for step, (x, y) in enumerate(dataloaders_train):

            time_start = time.time()

            optimizer.zero_grad()

            x, y = x.to(device), y.to(device)

            pre_y = T_model(x)

            loss = criterion(pre_y.unsqueeze(2), y)
            loss.backward()

            torch.nn.utils.clip_grad_norm_(T_model.parameters(), 0.1)  # 梯度裁剪，放backward和step直接
            optimizer.step()

            with warmup_scheduler.dampening():
                scheduler.step()

            train_loss += loss.item() * x.size(0)
            train_num += x.size(0)

            time_end = time.time()
            time_c = (time_end - time_start) * 100

            total_loss += loss.item()
            log_interval = int(len(trainX) / batch_size / 5)
            if (step + 1) % log_interval == 0 and (step + 1) > 0:
                cur_loss = total_loss / log_interval
                print('| epoch {:3d} | {:5d}/{:5d} batches | '
                      'lr {:02.6f} | '
                      'loss {:5.5f} | time {:8.2f}'.format(
                    epoch, (step + 1), len(trainX) // batch_size,scheduler.get_last_lr()[0],
                    cur_loss, time_c))
                total_loss = 0

        if (epoch + 1) % 5 == 0:
            print('-' * 89)
            print('end of epoch: {}, Loss:{:.7f}'.format(epoch + 1, loss.item()))
            print('-' * 89)
            train_loss_all.append(train_loss / train_num)

            # print_grad_norms(T_model)  ##

        # 验证阶段
        T_model.eval()
        val_loss = 0
        with torch.no_grad():
            for i, (data, target) in enumerate(dataloaders_valid):
                data, target = data.to(device), target.to(device)
                output = T_model(data)
                val_loss += criterion(output.unsqueeze(2), target).item()

        T_model.train()  # 将模型设置回train()模式



        print('Epoch: {} Validation Loss: {:.6f}'.format(epoch + 1, val_loss / train_num))

        if val_loss / train_num < best_val_loss:
            best_val_loss = val_loss / train_num
            best_model = T_model
            print('best_Epoch: {} Validation Loss: {:.6f}'.format(epoch + 1, best_val_loss))

        # scheduler.step()


    return best_model,best_val_loss




#%%
"""train"""
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)
input_dim = 11  # 输入大小
lat_num=128
# dec_seq_len=128
dec_seq_len=128
# d_model = 8
nhead = 4
num_encoder_layers = 2
num_decoder_layers = 2
dim_feedforward = 128 #input_dim 的倍数 i.e  4
dropout = 0.5
activation = 'relu'
out_seq_len = 1
#%%
TNN_dict = torch.load('4-SOC_prediction/models/single_balanced_TNN_B.pth')
model=[]
start = time.time()
for i in range(num_models):
    T_model = TransformerTS(input_dim,
                            dec_seq_len,
                            out_seq_len,
                            d_model=16,
                            nhead=nhead,
                            num_encoder_layers=num_encoder_layers,
                            num_decoder_layers=num_decoder_layers,
                            dim_feedforward=128,
                            dropout=0.5,
                            activation=activation,
                            custom_encoder=None,
                            custom_decoder=None).to(device)
    T_model.load_state_dict(TNN_dict)
    optimizer = torch.optim.AdamW(T_model.parameters(), lr=0.0001,
                              betas=(0.9, 0.999), weight_decay=1e-3)
    num_steps = len(dataloaders_train[i]) * 50
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)
    warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)
    criterion = nn.MSELoss()
    best_model, best_val_loss =train_model(T_model, dataloaders_train[i], dataloaders_valid,
                10, optimizer, criterion)
    model.append([best_model, best_val_loss])
    del T_model
    # torch.save(model[0][0].state_dict(),'4-SOC_prediction/models/ET_model{}.pth'.format(i))
    print(f"model: {i}")

end = time.time()

print (str(end-start))
#%%
# for i in range(num_models):
#     torch.save(model[i][0].state_dict(),
#                '4-SOC_prediction/models/BET_A{}.pth'.format(i))

for i in range(num_models):
    torch.save(model[i][0].state_dict(),
               '4-SOC_prediction/models/BET_B{}.pth'.format(i))

BET=model

#%%

"""load model"""
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)
input_dim = 11  # 输入大小
dec_seq_len=128
nhead = 4
num_encoder_layers = 2
num_decoder_layers = 2
dim_feedforward = 128 #input_dim 的倍数 i.e  4
activation = 'relu'
out_seq_len = 1
BET=[]
for i in range(5):
    T_model = TransformerTS(input_dim,
                            dec_seq_len,
                            out_seq_len,
                            d_model=8,
                            nhead=nhead,
                            num_encoder_layers=num_encoder_layers,
                            num_decoder_layers=num_decoder_layers,
                            dim_feedforward=128,
                            dropout=0.5,
                            activation=activation,
                            custom_encoder=None,
                            custom_decoder=None).to(device)
    T_model_dict = torch.load('4-SOC_prediction/models/BET_A{}.pth'.format(i))
    T_model.load_state_dict(T_model_dict)
    BET.append(T_model)
#%%

"""load model"""
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)
input_dim = 11  # 输入大小
dec_seq_len=128
nhead = 4
num_encoder_layers = 2
num_decoder_layers = 2
dim_feedforward = 128 #input_dim 的倍数 i.e  4
activation = 'relu'
out_seq_len = 1
BET=[]
for i in range(5):
    T_model = TransformerTS(input_dim,
                            dec_seq_len,
                            out_seq_len,
                            d_model=16,
                            nhead=nhead,
                            num_encoder_layers=num_encoder_layers,
                            num_decoder_layers=num_decoder_layers,
                            dim_feedforward=128,
                            dropout=0.5,
                            activation=activation,
                            custom_encoder=None,
                            custom_decoder=None).to(device)
    T_model_dict = torch.load('4-SOC_prediction/models/BET_B{}.pth'.format(i))
    T_model.load_state_dict(T_model_dict)
    BET.append(T_model)
#%%
sample_snapshot=samples1+samples2+samples3+samples4+samples5

# car,RDR_std,RDR_mean = feature_formation(sample_snapshot, ave_ECR_A, Capacity_A, Range_limt_A, 1)
car,RDR_std,RDR_mean = feature_formation(sample_snapshot, ave_ECR_B, Capacity_B, Range_limt_B, 2)

trainX1 = torch.tensor(car[:, 1:]).unsqueeze(2).float()
trainY1 = torch.tensor(car[:, 0]).unsqueeze(1).unsqueeze(2).float()
train_dataset1 = Data.TensorDataset(trainX1, trainY1)

car_test = Data.DataLoader(dataset=train_dataset1,
                           batch_size=1, shuffle=False)

max_val = trainY1.max().item()
min_val = trainY1.min().item()
interval_width = (max_val - min_val) / 5

all_simu = []
all_real = []
for i, (x, y) in enumerate(car_test):
    interval_index = int((y - min_val) // interval_width)
    interval_index = min(interval_index, 4)  # 将 interval_index 大于 4 的值修改为 4

    ET = BET[interval_index].eval()

    pred = ET(x.to(device)).float()

    # pred = sum_pred / num_models
    Norm_pred = pred.data.cpu().numpy()
    all_simu.append(Norm_pred)
    all_real.append(y.squeeze(1))


Preds = np.vstack(all_simu) * RDR_std + RDR_mean
GT = np.vstack(all_real) * RDR_std + RDR_mean
evaluation(GT, Preds)




